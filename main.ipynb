{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from functools import reduce\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loaders as l\n",
    "import models as m\n",
    "import train_eval as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/33783/Desktop/start_deep/start_deep/train_images'\n",
    "test_dir = 'C:/Users/33783/Desktop/start_deep/start_deep/test_images'\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(), \n",
    "     transforms.ToTensor(), \n",
    "     transforms.Normalize(mean=(0,),std=(1,))])\n",
    "\n",
    "valid_size = 0.2\n",
    "batch_size = 32\n",
    "\n",
    "train_loader,valid_loader,test_loader=l.make_all_loaders(train_dir,test_dir,transform,valid_size,batch_size)\n",
    "classes = ('noface','face')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "[ 1, 2000] loss: 0.157 f1_score on validation set : 0.946\n",
      "[ 2, 2000] loss: 0.035 f1_score on validation set : 0.968\n",
      "[ 3, 2000] loss: 0.021 f1_score on validation set : 0.976\n",
      "[ 4, 2000] loss: 0.017 f1_score on validation set : 0.981\n",
      "[ 5, 2000] loss: 0.012 f1_score on validation set : 0.984\n",
      "[ 6, 2000] loss: 0.010 f1_score on validation set : 0.986\n",
      "[ 7, 2000] loss: 0.009 f1_score on validation set : 0.988\n",
      "[ 8, 2000] loss: 0.007 f1_score on validation set : 0.989\n",
      "[ 9, 2000] loss: 0.007 f1_score on validation set : 0.990\n",
      "[10, 2000] loss: 0.005 f1_score on validation set : 0.991\n",
      "[11, 2000] loss: 0.005 f1_score on validation set : 0.991\n",
      "[12, 2000] loss: 0.005 f1_score on validation set : 0.992\n",
      "[13, 2000] loss: 0.005 f1_score on validation set : 0.992\n",
      "[14, 2000] loss: 0.004 f1_score on validation set : 0.993\n",
      "[15, 2000] loss: 0.003 f1_score on validation set : 0.993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2559a8fbee0>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = m.BaseNet()\n",
    "print(net)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#l'original mais moins bonne #optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "epochs=15\n",
    "\n",
    "#Training\n",
    "\n",
    "all_labels,all_predicted,all_losses,all_accuracies,all_f1scores= te.train_v1(net,criterion,optimizer,epochs,train_loader,valid_loader)\n",
    "            \n",
    "plt.plot(all_losses, color='blue')\n",
    "plt.plot(all_f1scores, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path='C:/Users/33783/Desktop/start_deep/start_deep/firstModel'\n",
    "torch.save(net.state_dict(), saved_model_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = m.BaseNet()\n",
    "net.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set\n",
      "f1_score 0.9923643485095572\n",
      "precision 0.9922769585828031\n",
      "recall 0.9924517776423092\n",
      "confusion matrix\n",
      " [[9.92959763e-01 6.97613129e-03 3.98880286e-05 2.42177317e-05]\n",
      " [7.53236609e-03 9.92409559e-01 3.33932961e-05 2.46820014e-05]\n",
      " [9.09090909e-02 9.09090909e-01 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9923643485095572,\n",
       " 0.9922769585828031,\n",
       " 0.9924517776423092,\n",
       " array([[9.92959763e-01, 6.97613129e-03, 3.98880286e-05, 2.42177317e-05],\n",
       "        [7.53236609e-03, 9.92409559e-01, 3.33932961e-05, 2.46820014e-05],\n",
       "        [9.09090909e-02, 9.09090909e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.calc_metrics_v1(net,test_loader,all_labels,all_predicted,show=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 93 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
