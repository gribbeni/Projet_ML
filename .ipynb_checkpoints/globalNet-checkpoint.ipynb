{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from functools import reduce\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loaders as l\n",
    "import models as m\n",
    "import train_eval as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ImageClassification import helpers as h\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model_path='C:/Users/33783/Desktop/start_deep/start_deep/Projet_ML/firstModel'\n",
    "net = m.BaseNet_v2()\n",
    "net.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 220.0, 40.0, tensor([[5.1205e-02, 9.4880e-01, 4.9572e-28, 3.1540e-28, 2.3270e-27, 8.2621e-28,\n",
      "         4.8853e-28, 1.4259e-28, 2.5784e-27, 4.4294e-28]])]\n",
      "[0, 220.0, 60.0, tensor([[2.5965e-01, 7.4035e-01, 1.7603e-19, 1.1718e-19, 2.5557e-19, 2.2960e-19,\n",
      "         1.4663e-19, 7.9271e-20, 3.2466e-19, 1.2384e-19]])]\n",
      "[0, 120.0, 80.0, tensor([[1.0657e-02, 9.8934e-01, 1.8039e-19, 1.9200e-19, 5.4302e-19, 1.5281e-19,\n",
      "         1.7553e-19, 8.6380e-20, 4.2498e-19, 1.6179e-19]])]\n",
      "[0, 60.0, 120.0, tensor([[2.0565e-04, 9.9979e-01, 1.4548e-15, 1.6965e-15, 2.6181e-15, 1.9600e-15,\n",
      "         1.0955e-15, 8.2156e-16, 2.8865e-15, 1.6420e-15]])]\n",
      "[0, 160.0, 180.0, tensor([[8.3464e-02, 9.1654e-01, 1.5664e-15, 1.9197e-15, 1.1625e-14, 4.1403e-15,\n",
      "         1.0703e-15, 6.2621e-16, 9.9875e-15, 1.5257e-15]])]\n",
      "[0, 160.0, 200.0, tensor([[5.6973e-02, 9.4303e-01, 9.7499e-15, 1.4479e-14, 7.5500e-14, 3.0845e-14,\n",
      "         1.0301e-14, 8.7022e-15, 7.9394e-14, 1.6793e-14]])]\n",
      "[1, 0.0, 24.0, tensor([[4.8035e-01, 5.1959e-01, 8.3730e-06, 7.5737e-06, 5.4414e-06, 8.3988e-06,\n",
      "         9.1919e-06, 8.0790e-06, 8.4771e-06, 6.7036e-06]])]\n",
      "[1, 120.0, 48.0, tensor([[1.2876e-03, 9.9871e-01, 2.8753e-19, 3.0283e-19, 1.2279e-18, 4.4030e-19,\n",
      "         2.9902e-19, 1.1881e-19, 1.1931e-18, 2.8033e-19]])]\n",
      "[1, 216.0, 48.0, tensor([[1.3577e-08, 1.0000e+00, 5.5483e-24, 8.3648e-24, 9.8964e-24, 8.9336e-24,\n",
      "         4.6622e-24, 2.4241e-24, 1.5104e-23, 4.5400e-24]])]\n",
      "[1, 48.0, 120.0, tensor([[2.1562e-01, 7.8438e-01, 1.7799e-16, 2.8401e-16, 1.1572e-15, 7.8227e-16,\n",
      "         1.4536e-16, 1.0366e-16, 1.5046e-15, 3.6873e-16]])]\n",
      "[1, 192.0, 192.0, tensor([[3.1479e-01, 6.8521e-01, 2.5172e-12, 3.4903e-12, 6.7665e-13, 1.7528e-12,\n",
      "         2.9646e-12, 3.2047e-12, 9.7393e-13, 2.1503e-12]])]\n",
      "[2, 115.19999999999999, 28.799999999999997, tensor([[1.9972e-01, 8.0028e-01, 2.7543e-17, 3.7479e-17, 4.2867e-17, 7.6218e-17,\n",
      "         2.0635e-17, 7.0510e-18, 8.6386e-17, 2.8168e-17]])]\n",
      "[2, 201.6, 28.799999999999997, tensor([[2.7758e-04, 9.9972e-01, 4.9214e-24, 1.1419e-23, 1.0206e-22, 3.3398e-23,\n",
      "         4.0514e-24, 5.2746e-24, 9.2964e-23, 2.0984e-23]])]\n",
      "[2, 115.19999999999999, 57.599999999999994, tensor([[6.1342e-02, 9.3866e-01, 8.3091e-19, 8.7319e-19, 3.8804e-18, 1.9487e-18,\n",
      "         6.0518e-19, 3.5548e-19, 4.7681e-18, 8.5020e-19]])]\n",
      "[2, 172.79999999999998, 57.599999999999994, tensor([[1.9604e-05, 9.9998e-01, 2.7011e-13, 2.8009e-13, 1.2817e-13, 3.1134e-13,\n",
      "         1.7374e-13, 1.9040e-13, 1.9208e-13, 1.8840e-13]])]\n",
      "[3, 103.67999999999998, 103.67999999999998, tensor([[3.4995e-01, 6.5005e-01, 9.5681e-18, 1.3269e-17, 1.3786e-17, 1.3745e-17,\n",
      "         5.2392e-18, 5.4423e-18, 1.8927e-17, 9.8919e-18]])]\n",
      "[4, 0.0, 82.94399999999999, tensor([[3.2121e-01, 6.7879e-01, 2.8805e-17, 1.2973e-17, 2.5947e-17, 3.4450e-17,\n",
      "         1.6716e-17, 1.4737e-17, 3.6481e-17, 1.8098e-17]])]\n",
      "[4, 41.471999999999994, 124.416, tensor([[3.3460e-01, 6.6540e-01, 2.6707e-18, 2.9486e-18, 1.0888e-17, 8.7932e-18,\n",
      "         1.6029e-18, 1.7206e-18, 1.2797e-17, 4.7678e-18]])]\n",
      "[4, 82.94399999999999, 124.416, tensor([[7.0643e-04, 9.9929e-01, 3.5001e-14, 4.5204e-14, 1.9357e-14, 4.1638e-14,\n",
      "         3.8024e-14, 2.7206e-14, 2.0371e-14, 3.0570e-14]])]\n",
      "[4, 165.88799999999998, 165.88799999999998, tensor([[1.6627e-04, 9.9983e-01, 9.8739e-15, 9.8897e-15, 1.1078e-14, 1.2610e-14,\n",
      "         1.0373e-14, 7.0320e-15, 1.7247e-14, 7.8398e-15]])]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# load the image and define the window width and height\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "image = cv2.imread(\"C:/Users/33783/Desktop/start_deep/start_deep/Projet_ML/test4.jpg\")\n",
    "(winW, winH) = (36, 36)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(), \n",
    "     transforms.ToTensor(), \n",
    "     transforms.Normalize(mean=(0,),std=(1,))])\n",
    "face_detected=0\n",
    "layer=0\n",
    "color_face=(255,0,0)\n",
    "color_noFace=(0, 255, 0)\n",
    "sleep=0.0015\n",
    "detected_faces=[]\n",
    "sc=1.2\n",
    "\n",
    "# loop over the image pyramid\n",
    "for resized in h.pyramid(image, scale=sc):\n",
    "    \n",
    "    # loop over the sliding window for each layer of the pyramid\n",
    "    \n",
    "    \n",
    "    for (x, y, window) in h.sliding_window(resized, stepSize=20, windowSize=(winW, winH)):\n",
    "        # if the window does not meet our desired window size, ignore it\n",
    "        if window.shape[0] != winH or window.shape[1] != winW:\n",
    "            continue\n",
    "\n",
    "        # THIS IS WHERE YOU WOULD PROCESS YOUR WINDOW, SUCH AS APPLYING A\n",
    "        # MACHINE LEARNING CLASSIFIER TO CLASSIFY THE CONTENTS OF THE\n",
    "        # WINDOW\n",
    "        color=color_noFace\n",
    "        sleep=0.005\n",
    "\n",
    "        im = Image.fromarray(window.astype('uint8'), 'RGB')\n",
    "        im=transform(im)\n",
    "        im = im.unsqueeze(0)\n",
    "        outputs=net(im)\n",
    "        prob=torch.nn.functional.softmax(outputs, dim=1).detach()#[0][1]\n",
    "        #print(prob)\n",
    "        y_pred_softmax = torch.log_softmax(outputs, dim = 1)\n",
    "        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "        prediction=np.array(y_pred_tags)\n",
    " \n",
    "        new_face=[]\n",
    "        if prediction==1:\n",
    "            face_detected+=1\n",
    "            new_face.append(layer)\n",
    "            new_face.append(x*pow(1.2,layer))\n",
    "            new_face.append(y*pow(1.2,layer))\n",
    "            new_face.append((prob))\n",
    "            detected_faces.append(new_face)\n",
    "            print(new_face)\n",
    "            color=color_face\n",
    "            sleep=0.5\n",
    "        \n",
    "            \n",
    "        #draw the window\n",
    "        clone = resized.copy()\n",
    "        cv2.rectangle(clone, (x, y), (x + winW, y + winH), color, 2)\n",
    "        cv2.imshow(\"Window\", clone)\n",
    "        cv2.waitKey(1)\n",
    "        time.sleep(sleep)\n",
    "        \n",
    "    layer+=1\n",
    "        \n",
    "print(face_detected)\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.destroyAllWindows() \n",
    "nb_layers=detected_faces[-1][0]\n",
    "\n",
    "\n",
    "for i in detected_faces : \n",
    "\n",
    "\n",
    "    x=i[1]+(winW*pow(1.2,i[0])/2)\n",
    "    y=i[2]+(winH*pow(1.2,i[0])/2)\n",
    "\n",
    "    #cv2.circle(image, (int(x), int(y)), 1,color_face, 3)\n",
    "\n",
    "    cv2.rectangle(image, (int(i[1]), int(i[2])), (int(i[1]) +int(winW*pow(1.2,i[0])), int(i[2]) + int(winH*pow(1.2,i[0]))), color_face, 2)\n",
    "\n",
    "cv2.imshow(\"Layer\", image)\n",
    "\n",
    "cv2.waitKey(1)\n",
    "#cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
